{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif, RFE, SelectFromModel\nfrom sklearn.ensemble import RandomForestClassifier\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\npd.options.display.max_columns = None\npd.options.mode.chained_assignment = None  # default='warn'\nplt.rcParams['axes.grid'] = True","metadata":{"execution":{"iopub.status.busy":"2021-12-29T09:46:42.449163Z","iopub.execute_input":"2021-12-29T09:46:42.449489Z","iopub.status.idle":"2021-12-29T09:46:42.459949Z","shell.execute_reply.started":"2021-12-29T09:46:42.449457Z","shell.execute_reply":"2021-12-29T09:46:42.458842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data Anlaysis :\n##### Main aim is to understand more about the data.\n- Categorical Variables: \n    - How many.\n    - Cardinality of Categorical Variables (bar): to decide later which to use label encoding or one-hot encoding\n    \n- Numerical Variables:\n   - How many.\n   - Distribution of the Numerical Variables (histogram,line): use this information later at the preoprocessing stage.\n      - Data skewed left\\right => data has outliers or has many missing values.\n      - The goal is to convert non noraml distribution to more like normal one.\n      - The converting is done by: cleaning data, removing outliers, filling missing values with median if outliers exist, etc.\n\n- All Features:\n    - Missing Values: calculate NAN% in each var to decide whether to drop the feature or filling it with the proper strategy\n    - Duplicates\n    - Outliers: detect Outliers with(boxplot, IQR) \n    - Relationship between independent and dependent feature(machine_status): gonna postpone this step till later after label encoding the target and setting timestamp as our data index","metadata":{}},{"cell_type":"code","source":"dataset = pd.read_csv(\"/kaggle/input/pump-sensor-data/sensor.csv\")\ndataset.shape","metadata":{"execution":{"iopub.status.busy":"2021-12-29T09:46:42.462223Z","iopub.execute_input":"2021-12-29T09:46:42.46264Z","iopub.status.idle":"2021-12-29T09:46:46.09527Z","shell.execute_reply.started":"2021-12-29T09:46:42.462589Z","shell.execute_reply":"2021-12-29T09:46:46.094422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.head(5)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-12-29T09:46:46.096172Z","iopub.execute_input":"2021-12-29T09:46:46.096382Z","iopub.status.idle":"2021-12-29T09:46:46.159238Z","shell.execute_reply.started":"2021-12-29T09:46:46.096358Z","shell.execute_reply":"2021-12-29T09:46:46.158302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.dtypes","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-12-29T09:46:46.160283Z","iopub.execute_input":"2021-12-29T09:46:46.160498Z","iopub.status.idle":"2021-12-29T09:46:46.169944Z","shell.execute_reply.started":"2021-12-29T09:46:46.160472Z","shell.execute_reply":"2021-12-29T09:46:46.168935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#We'll have to convert timestamp to datetime and maybe index our data on it\n#also we may have to categories the target 'machine_status'","metadata":{"execution":{"iopub.status.busy":"2021-12-29T09:46:46.17287Z","iopub.execute_input":"2021-12-29T09:46:46.173286Z","iopub.status.idle":"2021-12-29T09:46:46.179488Z","shell.execute_reply.started":"2021-12-29T09:46:46.173224Z","shell.execute_reply":"2021-12-29T09:46:46.178593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Categorical variables:\n- How many?\n- Cardinality of each feature: to decide later which to use label encoding or one-hot encoding (use bar chart)","metadata":{}},{"cell_type":"code","source":"dataset.describe(include='O')","metadata":{"execution":{"iopub.status.busy":"2021-12-29T09:46:46.181169Z","iopub.execute_input":"2021-12-29T09:46:46.181666Z","iopub.status.idle":"2021-12-29T09:46:46.50204Z","shell.execute_reply.started":"2021-12-29T09:46:46.181577Z","shell.execute_reply":"2021-12-29T09:46:46.500967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"count only counts non-null values.\ntimestamp: count = unique ==> no nulls + no duplicate ==> use timestamp as our data index later at the data preprocessing stage","metadata":{}},{"cell_type":"code","source":"sns.countplot(x=dataset['machine_status'])","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-12-29T09:46:46.50344Z","iopub.execute_input":"2021-12-29T09:46:46.503672Z","iopub.status.idle":"2021-12-29T09:46:46.955315Z","shell.execute_reply.started":"2021-12-29T09:46:46.503644Z","shell.execute_reply":"2021-12-29T09:46:46.954682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#List machine_status distinct values:\ndataset['machine_status'].unique().tolist()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-12-29T09:46:46.956229Z","iopub.execute_input":"2021-12-29T09:46:46.956987Z","iopub.status.idle":"2021-12-29T09:46:46.979169Z","shell.execute_reply.started":"2021-12-29T09:46:46.956956Z","shell.execute_reply":"2021-12-29T09:46:46.978485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#check frequency of each value\ndataset['machine_status'].value_counts()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-12-29T09:46:46.980391Z","iopub.execute_input":"2021-12-29T09:46:46.981159Z","iopub.status.idle":"2021-12-29T09:46:47.024734Z","shell.execute_reply.started":"2021-12-29T09:46:46.981115Z","shell.execute_reply":"2021-12-29T09:46:47.023744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"machine_status: 3 distinct values, no nulls ==> use label-encoding since we only have 3 distict values","metadata":{}},{"cell_type":"markdown","source":"### Thoughts:\n\nThe water pumb got broken 7 times, which is a huge number for just one year.\n\nWe'll try to find out if there is any pattern that connects failure status to timestamp, does is it occur more at night time? early morning? afternoon? etc.\n\nOf course if we have got more data or if sensors were defined appropriately instead of labelling them just with sensor number, we would have a more clear vision of what's going on. but for now we'll do our best with the data we have as it is.","metadata":{}},{"cell_type":"code","source":"#get timestamp where status == broken\ntimestamp = dataset[['timestamp','machine_status']].loc[dataset['machine_status'].str.lower()=='broken']\ntimestamp.set_index('timestamp', inplace= True)\ntimestamp","metadata":{"execution":{"iopub.status.busy":"2021-12-29T09:46:47.02622Z","iopub.execute_input":"2021-12-29T09:46:47.026915Z","iopub.status.idle":"2021-12-29T09:46:47.199029Z","shell.execute_reply.started":"2021-12-29T09:46:47.026873Z","shell.execute_reply":"2021-12-29T09:46:47.198226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Thoughts:\n\nAll failues had happened twice almost for every month starting from April till July. except for june there was only one failure at the end of month. Could failure be releated to tempertaure rising during these months? The dataset owner is originated from Bangkok, so it's a safe guess to say the data is originated from Thailand. Summer in Thai starts from March to June, April being the hottest month in the year. The failures that occurred in July can be interpreted as the result of accumulations from June (assuming the stability of the electric current, it is logical to assume that the failure did not happen overnight, rather it was a gradual cumulative process). 85% of failures happenned between 10pm and 1am. These findings may have a meaning when presented to data owner.","metadata":{}},{"cell_type":"markdown","source":"### Numerical variables:\n- Numerical Variables:\n   - How many.\n   - Distribution of the Numerical Variables (histogram,line): use this information later at the preoprocessing stage.\n      - Data skewed left\\right => data has outliers or has many missing values.\n      - The goal is to convert non noraml distribution to more like normal one.\n      - The converting is done by: cleaning data, removing outliers, filling missing values with median if outliers exist, etc.","metadata":{}},{"cell_type":"code","source":"dataset.describe().transpose()","metadata":{"execution":{"iopub.status.busy":"2021-12-29T09:46:47.200284Z","iopub.execute_input":"2021-12-29T09:46:47.200585Z","iopub.status.idle":"2021-12-29T09:46:47.700448Z","shell.execute_reply.started":"2021-12-29T09:46:47.200555Z","shell.execute_reply":"2021-12-29T09:46:47.699649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have 52 continuous numerical features.\nwe suspect that the Unnamed: 0 and sensor_15 are both meaningless, the first is for numbering, the second is empty. \nwe'll further investigate both.","metadata":{}},{"cell_type":"code","source":"print('duplicates= %d' % dataset['Unnamed: 0'].duplicated().sum())\ndataset['Unnamed: 0'].describe()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-12-29T09:46:47.701652Z","iopub.execute_input":"2021-12-29T09:46:47.701893Z","iopub.status.idle":"2021-12-29T09:46:47.720278Z","shell.execute_reply.started":"2021-12-29T09:46:47.701864Z","shell.execute_reply":"2021-12-29T09:46:47.719507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"No Duplicates, min=0, max= 220319 ==> Obviously it's used as an index. We'll replace it later with timestamp.","metadata":{}},{"cell_type":"code","source":"print('duplicates= %d' % dataset['sensor_15'].duplicated().sum())\ndataset['sensor_15'].describe()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-12-29T09:46:47.721387Z","iopub.execute_input":"2021-12-29T09:46:47.721719Z","iopub.status.idle":"2021-12-29T09:46:47.740051Z","shell.execute_reply.started":"2021-12-29T09:46:47.721692Z","shell.execute_reply":"2021-12-29T09:46:47.739166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Safe to drop sensor_15 since it's null feature\n### Data Distribution:","metadata":{}},{"cell_type":"code","source":"##Group numerical features in one temporary dataframe for further analysis:\nNumerical_features = dataset.select_dtypes(exclude=['object'])\n# Since sensor data are continues values, we use histogram to understand the distribution\nfor feature in Numerical_features:\n    data=dataset.copy()\n    data[feature].hist(bins=25)\n    plt.xlabel(feature)\n    plt.ylabel(\"Count\")\n    plt.title(feature)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-29T09:46:47.743265Z","iopub.execute_input":"2021-12-29T09:46:47.74349Z","iopub.status.idle":"2021-12-29T09:47:01.972157Z","shell.execute_reply.started":"2021-12-29T09:46:47.743464Z","shell.execute_reply":"2021-12-29T09:47:01.970463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that we have the three distributions; i.e., normal, skewed left\\right.\nThe distribution should be semi\\normal after handling missing data and outliers, which we'll do at the preprocessing stage.\nWe also see that some features have alomst the same distribution. one can attribute this to the fact that in actual operation, pump systems are equipped with multi sensors for the same operating parameter, such as pressure, temperature, etc. for a couple of reasons such as system reliability, safety and so on. \nThis also can cause measured signals' overlapping as seen next:","metadata":{}},{"cell_type":"code","source":"dataset.plot(subplots =True, sharex = True, figsize = (20,50))","metadata":{"execution":{"iopub.status.busy":"2021-12-29T09:47:01.973204Z","iopub.execute_input":"2021-12-29T09:47:01.973399Z","iopub.status.idle":"2021-12-29T09:47:35.228431Z","shell.execute_reply.started":"2021-12-29T09:47:01.973375Z","shell.execute_reply":"2021-12-29T09:47:35.227602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Try to detect trends in sensors when the machine gets broken\n#This will help us in feature selection later\ndf = dataset.drop(['Unnamed: 0','sensor_15'], axis=1)\ndf['timestamp'] = pd.to_datetime(df['timestamp'])\ndf.set_index('timestamp', inplace= True)\n\ndf_broken = df[df['machine_status']=='BROKEN']\ndf_sensors = df.drop(['machine_status'], axis=1)\nsensors = df_sensors.columns\n\nfor sensor in sensors:\n    sns.set_context('paper')\n    _ = plt.figure(figsize=(16,2))\n    _ = plt.plot(df[sensor], color='green')\n    _ = plt.plot(df_broken[sensor], linestyle='none', marker='X', color='red', markersize=8)   \n    _ = plt.title(sensor)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-29T09:47:35.229924Z","iopub.execute_input":"2021-12-29T09:47:35.230244Z","iopub.status.idle":"2021-12-29T09:47:49.318632Z","shell.execute_reply.started":"2021-12-29T09:47:35.230209Z","shell.execute_reply":"2021-12-29T09:47:49.318013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It seems that sensors <b>{0,4,5,6,7,10,11,12,13}</b> correlate a lot with the failure of the machine and can be a good indicator of the failure of the system. We'll keep this in mind when we select features later.","metadata":{}},{"cell_type":"code","source":"#Plot the whole raw dataset to detect correlation between sensors and target, also multicollinearity if any. \n#This information also will help us in feature selecting later.\n#label encode machine status to be able to plot it along with the other numeric sensors\n\ndf = dataset.drop(['Unnamed: 0','sensor_15'], axis=1)\ndf['timestamp'] = pd.to_datetime(df['timestamp'])\ndf.set_index('timestamp', inplace= True)\n\ndf['machine_status'] = df['machine_status'].map({'NORMAL': 0, 'BROKEN': 1, 'RECOVERING':1})\n\n#df.dtypes\ncorr = df.corr()\nplt.figure(figsize=(24,24))\nsns.heatmap(corr, annot=True, fmt = '.1f')\n","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-12-29T09:47:49.319977Z","iopub.execute_input":"2021-12-29T09:47:49.32048Z","iopub.status.idle":"2021-12-29T09:48:04.915889Z","shell.execute_reply.started":"2021-12-29T09:47:49.320439Z","shell.execute_reply":"2021-12-29T09:48:04.915021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- There is a strong correlation exists between sensor_14 to sensor_26, sensor_28 to sensor_33 and sensor_34 to sensor_36.\n- Multicollinearity should be handled, otherwise ML model performance will decrease.\n- machine_status is highly negative correlated with sensor_01 to sensor_12.","metadata":{}},{"cell_type":"markdown","source":"### All Features:\n- Missing Values: calculate NAN% in each var to decide whether to drop the feature or filling it with the proper strategy\n- Duplicates\n- Outliers: detect Outliers with(boxplot, IQR) \n- Correlation between independent and dependent feature(machine_status)","metadata":{}},{"cell_type":"markdown","source":"### Finding Missing Values (NAN %):","metadata":{}},{"cell_type":"code","source":"#As we've seen above, we only have two categorical features; machine_status and timestamp.\n#Neither one of them has missing values. so here we'll concentrate only on numerical variables.\n#Get Null Precentage in each column. I think % is more readable, but the plain view is fine as well.\n\nfeatures_with_na=[features for features in Numerical_features.columns if Numerical_features[features].isna().sum()>1]\nna_percent = round(Numerical_features[features_with_na].isna().sum() * 100 / len(Numerical_features),2)\nmissing_percent_df = pd.DataFrame({'Feature': features_with_na, 'Na%':na_percent})\n\n#Sort based on null precentage in each feature\nmissing_percent_df.sort_values('Na%', inplace=True, ascending=False)\nmissing_percent_df","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-12-29T09:48:04.917427Z","iopub.execute_input":"2021-12-29T09:48:04.917891Z","iopub.status.idle":"2021-12-29T09:48:05.036603Z","shell.execute_reply.started":"2021-12-29T09:48:04.917855Z","shell.execute_reply":"2021-12-29T09:48:05.03568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Finding Duplicates:","metadata":{}},{"cell_type":"code","source":"#look for duplicated rows based on timestamp column\n#false == all rows are unique rows == no duplicate rows\nboolean = not dataset[\"timestamp\"].is_unique\nboolean","metadata":{"execution":{"iopub.status.busy":"2021-12-29T09:48:05.03768Z","iopub.execute_input":"2021-12-29T09:48:05.037892Z","iopub.status.idle":"2021-12-29T09:48:05.094723Z","shell.execute_reply.started":"2021-12-29T09:48:05.037867Z","shell.execute_reply":"2021-12-29T09:48:05.093844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Finding Outliers:","metadata":{}},{"cell_type":"code","source":"#since so many of our features have skewed distribution, we'll use IQR method to detect outliers in these features:\nfor feature in Numerical_features:  \n    series = Numerical_features[feature]\n    Q1 = series.quantile(0.25) \n    Q3 = series.quantile(0.75) \n    IQR = Q3 - Q1\n    lower = Q1 - (1.5 * IQR)\n    upper = Q3 + (1.5 * IQR)\n    outliers = [x for x in series if x < lower or x > upper]\n    print('Identified outliers: %d' % len(outliers))\n    outliers_removed = [x for x in series if x >= lower and x <= upper]\n    print('Non-outlier observations: %d' % len(outliers_removed))\n    print('Outliers in',feature,':', round(len(outliers) * 100 / len(series),2),'%')\n    plt.figure(figsize=(6, 3))\n    sns.boxplot(x= series)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-29T09:48:05.096015Z","iopub.execute_input":"2021-12-29T09:48:05.097332Z","iopub.status.idle":"2021-12-29T09:48:29.286186Z","shell.execute_reply.started":"2021-12-29T09:48:05.09728Z","shell.execute_reply":"2021-12-29T09:48:29.285245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Thoughts:\n\nWe know that our numeric features are sensors data, and we expected there will be outliers due to the fact that these sensors are working in a real enviroment and it is inevitable that some sensor nodes malfunction, which may result in noisy, faulty, missing and redundant data. Also the low cost and low quality sensor nodes have firm resource constraints such as energy (battery power), memory, computational capacity and communication bandwidth. The limited resource and capability affect the reliablity and the accuracy of the generated data. Especially when battery power is exhausted, the probability of generating erroneous data will grow rapidly. \n\nEven With that in mind, we're not sure if what appear as outliers in the plots are really outliers.\nCould they be just part of the data? ","metadata":{}},{"cell_type":"markdown","source":"### Data Preprocessing\n- Drop obvious null or irrelevant features\n- Re-index dataset if needed\n- Encode Label data: one-hot or label-encoding (in our case we choose label-enoding)\n- Data Imputation: \n    - if numerical => constant Or statistics.\n    - if categorical => constant or most-frequent.\n    - if data has outliers, then median should be used as both mean and std. are affected greatly with the outliers.\n- Handle Outliers by (we chose capping technique as we don't want to just trim them) \n- Data Scaling: we chose Robust Scaler just in case we still have outliers\n- Remove Duplicates.\n- Dataset Correlation\n- Correlation between independent and target feature(machine_status): use this information later in feature selection","metadata":{}},{"cell_type":"code","source":"#drop sensor_15, Unnamed: 0\ndataset.drop({'sensor_15','Unnamed: 0'},axis=1, inplace=True)\n\n#convert timestamp to datetime \ndataset['timestamp'] = pd.to_datetime(dataset['timestamp'])\n\n#set timestamp as data index\ndataset.set_index('timestamp', inplace= True)","metadata":{"execution":{"iopub.status.busy":"2021-12-29T09:48:29.287434Z","iopub.execute_input":"2021-12-29T09:48:29.287682Z","iopub.status.idle":"2021-12-29T09:48:29.401035Z","shell.execute_reply.started":"2021-12-29T09:48:29.287655Z","shell.execute_reply":"2021-12-29T09:48:29.400309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Label Encoding:","metadata":{}},{"cell_type":"code","source":"#Replace the categorical value with a numeric value. \n#NORMAL is mapped to 0, RECOVERING and BROKEN are mapped to 1.\n#We'll ue mapping here instead of LabelEncoder\n\ndataset['machine_status'] = dataset['machine_status'].map({'NORMAL': 0, 'BROKEN': 1, 'RECOVERING':1})","metadata":{"execution":{"iopub.status.busy":"2021-12-29T09:48:29.402177Z","iopub.execute_input":"2021-12-29T09:48:29.402593Z","iopub.status.idle":"2021-12-29T09:48:29.435504Z","shell.execute_reply.started":"2021-12-29T09:48:29.402559Z","shell.execute_reply":"2021-12-29T09:48:29.434604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset['machine_status'].value_counts()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-12-29T09:48:29.436791Z","iopub.execute_input":"2021-12-29T09:48:29.437355Z","iopub.status.idle":"2021-12-29T09:48:29.445939Z","shell.execute_reply.started":"2021-12-29T09:48:29.437312Z","shell.execute_reply":"2021-12-29T09:48:29.44496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data Imputation\nFrom plotting the features early in the EDA stage, clearly many of our data are skewed, plus there are many outlies. therefore we will use median value to fill in the missing values:","metadata":{}},{"cell_type":"code","source":"#Filling missing numerical values with median\n#first drop sensor_15 from features_with_na list\nfeatures_with_na.remove('sensor_15')\nmedian_series = dataset[features_with_na].median()\ndataset[features_with_na] = dataset[features_with_na].fillna(median_series)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-12-29T09:48:29.447601Z","iopub.execute_input":"2021-12-29T09:48:29.448076Z","iopub.status.idle":"2021-12-29T09:48:29.741916Z","shell.execute_reply.started":"2021-12-29T09:48:29.448031Z","shell.execute_reply":"2021-12-29T09:48:29.741052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Double check the whole dataframe again\nna_values = dataset[features_with_na].isna().sum()\nnot all(na_values) #If true ==> no missing values anymore.","metadata":{"execution":{"iopub.status.busy":"2021-12-29T09:48:29.743264Z","iopub.execute_input":"2021-12-29T09:48:29.743575Z","iopub.status.idle":"2021-12-29T09:48:29.803079Z","shell.execute_reply.started":"2021-12-29T09:48:29.743537Z","shell.execute_reply":"2021-12-29T09:48:29.802188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Handle Outliers (Via Capping):","metadata":{}},{"cell_type":"code","source":"#1. Cap ouliers in all features\nfor feature in dataset[features_with_na]:  \n    percentiles = dataset[feature].quantile([0.25, 0.75]).values\n    dataset[feature][dataset[feature] <= percentiles[0]] = percentiles[0]\n    dataset[feature][dataset[feature] >= percentiles[1]] = percentiles[1]","metadata":{"execution":{"iopub.status.busy":"2021-12-29T09:48:29.804443Z","iopub.execute_input":"2021-12-29T09:48:29.804761Z","iopub.status.idle":"2021-12-29T09:48:30.381816Z","shell.execute_reply.started":"2021-12-29T09:48:29.804703Z","shell.execute_reply":"2021-12-29T09:48:30.380927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#2. plot features after capping to see the effect\nfor feature in dataset[features_with_na]:    \n    Q1 = dataset[feature].quantile(0.25) \n    Q3 = dataset[feature].quantile(0.75) \n    IQR = Q3 - Q1\n    lower = Q1 - (1.5 * IQR)\n    upper = Q3 + (1.5 * IQR)\n    outliers = [y for y in dataset[feature] if y < lower or y > upper]\n    print('Outliers in',feature,':', round(len(outliers) * 100 / len(dataset[feature]),2),'%')\n    plt.figure(figsize=(6, 3))\n    sns.boxplot(x= dataset[feature])\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-29T09:48:30.382963Z","iopub.execute_input":"2021-12-29T09:48:30.383208Z","iopub.status.idle":"2021-12-29T09:48:42.98724Z","shell.execute_reply.started":"2021-12-29T09:48:30.383179Z","shell.execute_reply":"2021-12-29T09:48:42.986219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data Scaling:\nOur features varies largly in their scales, therefore scaling them is a neccessity to prepare our data to be fed to ML model later.\nWe'll use Robust Scaler.","metadata":{}},{"cell_type":"code","source":"scaler = preprocessing.RobustScaler()\ndataset[features_with_na] = scaler.fit_transform(dataset[features_with_na])\ndataset[features_with_na].describe()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-12-29T09:48:42.988877Z","iopub.execute_input":"2021-12-29T09:48:42.989361Z","iopub.status.idle":"2021-12-29T09:48:43.89998Z","shell.execute_reply.started":"2021-12-29T09:48:42.989315Z","shell.execute_reply":"2021-12-29T09:48:43.899052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Remove Duplicates:","metadata":{}},{"cell_type":"code","source":"dataset.drop_duplicates()","metadata":{"execution":{"iopub.status.busy":"2021-12-29T09:48:43.901162Z","iopub.execute_input":"2021-12-29T09:48:43.901416Z","iopub.status.idle":"2021-12-29T09:48:44.53532Z","shell.execute_reply.started":"2021-12-29T09:48:43.90139Z","shell.execute_reply":"2021-12-29T09:48:44.534461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Correlation:","metadata":{}},{"cell_type":"markdown","source":"1. Dataframe Correlation:\n- This knowledge help us to prepare our data to meet the expectations of ML algorithms, such as classification (which we're going to use here) whose performance will degrade with the presence of these interdependencies (multicollinearity).\n    \n    +1 ==> high positive correlation\n    \n    -1 ==> high negative correlation","metadata":{}},{"cell_type":"code","source":"corr = dataset.corr(method='spearman')\nplt.figure(figsize=(24,24))\nsns.heatmap(corr, annot=True, fmt = '.1f')","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-12-29T09:48:44.536749Z","iopub.execute_input":"2021-12-29T09:48:44.537474Z","iopub.status.idle":"2021-12-29T09:49:04.198874Z","shell.execute_reply.started":"2021-12-29T09:48:44.537426Z","shell.execute_reply":"2021-12-29T09:49:04.197953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Notice how scaling and outliers capping has affected the overall correlation.\n- Instead of the strong positive correlation between sensor_14 to sensor_26 that we saw when data was raw, now the correlation is average.\n- Same case with sensor_34 to sensor_36, sensor_22 to sensor_23 data.\n- machine_status now has a slightly negative correlation with features from sensor_01 to sensor_12.","metadata":{}},{"cell_type":"markdown","source":"2. Correlation between machine_status and independent features:","metadata":{}},{"cell_type":"code","source":"#Spearmanr is being used since most of our features don't follow a normal distribution\ncorr_matrix = dataset.corr(method='spearman')\nprint(corr_matrix[\"machine_status\"].sort_values(ascending=False))","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-12-29T09:49:04.200043Z","iopub.execute_input":"2021-12-29T09:49:04.20031Z","iopub.status.idle":"2021-12-29T09:49:10.295129Z","shell.execute_reply.started":"2021-12-29T09:49:04.200281Z","shell.execute_reply":"2021-12-29T09:49:10.294214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b>Group</b> features data based on machine_status mean and plot the result to get a better sense of feature \\ target relation ","metadata":{}},{"cell_type":"code","source":"for feature in dataset.columns:\n    dataset.groupby(feature)['machine_status'].mean().plot(legend=True)\n    plt.ylabel('machine_status')\n    plt.title(feature)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-29T09:49:10.296448Z","iopub.execute_input":"2021-12-29T09:49:10.297314Z","iopub.status.idle":"2021-12-29T09:49:29.500841Z","shell.execute_reply.started":"2021-12-29T09:49:10.29727Z","shell.execute_reply":"2021-12-29T09:49:29.499916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Feature Selection\nBefore applying feature selection method, we need to split the data first. The reason is that we only select features based on the information from the training set, not on the whole data set. We should hold out part of the whole data set as a test set to evaluate the performance of the feature selection and the model. Thus the information from the test set cannot be seen while we conduct feature selection and train the model.","metadata":{}},{"cell_type":"code","source":"#Capture the dependent feature\ny = dataset['machine_status']\n\n#Drop dependent feature from dataset\nX = dataset.drop('machine_status',axis=1)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=100, test_size=0.3)\n#We will apply the feature selection based on X_train and y_train","metadata":{"execution":{"iopub.status.busy":"2021-12-29T09:49:29.502449Z","iopub.execute_input":"2021-12-29T09:49:29.502704Z","iopub.status.idle":"2021-12-29T09:49:29.652824Z","shell.execute_reply.started":"2021-12-29T09:49:29.502665Z","shell.execute_reply":"2021-12-29T09:49:29.651888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have numerical features as input and categorical feature as an output.\nWith that in mind we'll test 4 selection methods and see what comes out!","metadata":{}},{"cell_type":"markdown","source":"### 1. ANOVA: f_classif()","metadata":{}},{"cell_type":"code","source":"# Univariate feature selection with F-test\n# computes ANOVA f-value\n# We select the most significant features\nsel_f = SelectKBest(f_classif)\nX_train_f = sel_f.fit_transform(X_train, y_train)\nprint(sel_f.get_support())","metadata":{"execution":{"iopub.status.busy":"2021-12-29T09:49:29.653981Z","iopub.execute_input":"2021-12-29T09:49:29.654532Z","iopub.status.idle":"2021-12-29T09:49:29.825103Z","shell.execute_reply.started":"2021-12-29T09:49:29.654495Z","shell.execute_reply":"2021-12-29T09:49:29.824198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feat_1 = X_train.columns[(sel_f.get_support())]\nprint('selected features: {}'.format(len(feat_1)))\nfeat_1","metadata":{"execution":{"iopub.status.busy":"2021-12-29T09:49:29.82642Z","iopub.execute_input":"2021-12-29T09:49:29.826661Z","iopub.status.idle":"2021-12-29T09:49:29.834099Z","shell.execute_reply.started":"2021-12-29T09:49:29.826624Z","shell.execute_reply":"2021-12-29T09:49:29.8332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2. Mutual Information: mutual_info_classif()","metadata":{}},{"cell_type":"code","source":"# Computes the mutual information\n# Mutual information between two random variables is a non-negative value, which measures the dependency between the variables. \n# It is equal to zero if and only if two random variables are independent, and higher values mean higher dependency.\n\nsel_mutual = SelectKBest(mutual_info_classif)\nX_train_mutual = sel_mutual.fit_transform(X_train, y_train)\nprint(sel_mutual.get_support())","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-12-29T09:49:29.835523Z","iopub.execute_input":"2021-12-29T09:49:29.835849Z","iopub.status.idle":"2021-12-29T09:51:16.262856Z","shell.execute_reply.started":"2021-12-29T09:49:29.835816Z","shell.execute_reply":"2021-12-29T09:51:16.261862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feat_2 = X_train.columns[(sel_mutual.get_support())]\nprint('selected features: {}'.format(len(feat_2)))\nfeat_2","metadata":{"execution":{"iopub.status.busy":"2021-12-29T09:51:16.264119Z","iopub.execute_input":"2021-12-29T09:51:16.26437Z","iopub.status.idle":"2021-12-29T09:51:16.273008Z","shell.execute_reply.started":"2021-12-29T09:51:16.264342Z","shell.execute_reply":"2021-12-29T09:51:16.272108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3. Recursive feature elimination (RFE): \n#### - Random forest as the model","metadata":{}},{"cell_type":"code","source":"# Select features by recursively considering smaller and smaller sets of features.\n# First, the estimator is trained on the initial set of features and the importance of each feature is obtained either \n# through a coef_ attribute or through a feature_importances_ attribute.\n# Then, the least important features are pruned from the current set of features. \n# That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.\n\nmodel_tree = RandomForestClassifier(random_state=100, n_estimators=50)\nsel_rfe_tree = RFE(estimator=model_tree, step=1)\nX_train_rfe_tree = sel_rfe_tree.fit_transform(X_train, y_train)\nprint(sel_rfe_tree.get_support())","metadata":{"execution":{"iopub.status.busy":"2021-12-29T09:51:16.274604Z","iopub.execute_input":"2021-12-29T09:51:16.275316Z","iopub.status.idle":"2021-12-29T09:55:24.102094Z","shell.execute_reply.started":"2021-12-29T09:51:16.275276Z","shell.execute_reply":"2021-12-29T09:55:24.101203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feat_3 = X_train.columns[(sel_rfe_tree.get_support())]\nprint('selected features: {}'.format(len(feat_3)))\nfeat_3","metadata":{"execution":{"iopub.status.busy":"2021-12-29T09:55:24.103411Z","iopub.execute_input":"2021-12-29T09:55:24.104178Z","iopub.status.idle":"2021-12-29T09:55:24.112696Z","shell.execute_reply.started":"2021-12-29T09:55:24.104134Z","shell.execute_reply":"2021-12-29T09:55:24.111854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4. Select From Model: SelectFromModel\n#### - Tree-based feature selection","metadata":{}},{"cell_type":"code","source":"# The features are considered unimportant and removed if the corresponding coef_ or feature_importances_ values are \n# below the provided threshold parameter.\n# Compared to univariate feature selection, model-based feature selection consider all feature at once,\n# thus can capture interactions. \n# The model used for the feature selection doesn’t need to be the same model for the training later.\n\nmodel_tree = RandomForestClassifier(random_state=100, n_estimators=50)\nmodel_tree.fit(X_train, y_train)\nprint(model_tree.feature_importances_)\nsel_model_tree = SelectFromModel(estimator=model_tree, prefit=True, threshold='mean')  \n      # since we already fit the data, we specify prefit option here\n      # Features whose importance is greater or equal to the threshold are kept while the others are discarded.\nX_train_sfm_tree = sel_model_tree.transform(X_train)\nprint(sel_model_tree.get_support())","metadata":{"execution":{"iopub.status.busy":"2021-12-29T09:55:24.114085Z","iopub.execute_input":"2021-12-29T09:55:24.114859Z","iopub.status.idle":"2021-12-29T09:55:36.119795Z","shell.execute_reply.started":"2021-12-29T09:55:24.114816Z","shell.execute_reply":"2021-12-29T09:55:36.118938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# make a list of the selected features\nfeat_4 = X_train.columns[(sel_model_tree.get_support())]\nprint('selected features: {}'.format(len(feat_4)))\nfeat_4","metadata":{"execution":{"iopub.status.busy":"2021-12-29T09:55:36.125103Z","iopub.execute_input":"2021-12-29T09:55:36.125355Z","iopub.status.idle":"2021-12-29T09:55:36.138692Z","shell.execute_reply.started":"2021-12-29T09:55:36.125326Z","shell.execute_reply":"2021-12-29T09:55:36.137749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally, we select features based on intersection between:\n- Correlation analysis\n- Anova: F-test\n- Mutual_info_classif test\n- Recursive feature elimination with random forest as the model\n- SelectFromModel: tree based feature selection","metadata":{}},{"cell_type":"code","source":"# Gonna intersect our four lists\nlist1_as_set = set(feat_1)\n\nintersect = list1_as_set.intersection(feat_2) #1,2\nintersect1 = intersect.intersection(feat_3) #2,3\nintersect2 = intersect1.intersection(feat_4) #3,4\n\nintersection_as_list = list(intersect2)\n\nintersection_as_list.sort()\nselected_features = intersection_as_list\nselected_features\n#0,4,5,6,7,10,11,12,13","metadata":{"execution":{"iopub.status.busy":"2021-12-29T09:55:36.139796Z","iopub.execute_input":"2021-12-29T09:55:36.140012Z","iopub.status.idle":"2021-12-29T09:55:36.147909Z","shell.execute_reply.started":"2021-12-29T09:55:36.139987Z","shell.execute_reply":"2021-12-29T09:55:36.147278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = X_train[selected_features]\nX_train.columns","metadata":{"execution":{"iopub.status.busy":"2021-12-29T09:55:36.148928Z","iopub.execute_input":"2021-12-29T09:55:36.149209Z","iopub.status.idle":"2021-12-29T09:55:36.165721Z","shell.execute_reply.started":"2021-12-29T09:55:36.149181Z","shell.execute_reply":"2021-12-29T09:55:36.164839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model Building:","metadata":{}},{"cell_type":"markdown","source":"We'll try multiple models and select the most accurate one. \nModels include: Logistic Regression - Decision Tree - Random Forest - Xgboost\n\n- Hyperparameter Tuning: it's required to get the most out of your Ml model. Random Search is used here.\n- tol, max_iter are considered as parameters and its values are tol=[0.0001,0.001,0.01,0.1], max_iter=[20,50,100,200]\n- tol represents tolerance for stopping criteria\n- max_iter represents maximum number of iterations taken for solvers to converge\n- Random search is the best parameter technique when there are less number of dimensions\n- Evaluation with Confusion Matrix + Classification Report\n- Model With Best Parameter\n- Apply the selected best parameters to Logistic Regression and fit the model\n- Predict the probability score using predict_proba() for train and test data\n- Plot the roc curve for train and test data\n","metadata":{}},{"cell_type":"markdown","source":"### I. Logistic Regression","metadata":{}},{"cell_type":"code","source":"from scipy.stats import loguniform\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import RandomizedSearchCV\n\n# define model\nmodel = LogisticRegression()\n# define evaluation\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n# define search space\nspace = dict() #solver. Use either ‘liblinear’ or ‘saga\nspace['solver'] = ['liblinear']\nspace['penalty'] = ['l1']\nspace['C'] = loguniform(1e-5, 100)\n# define search\nsearch = RandomizedSearchCV(model, space, n_iter=500, scoring='accuracy', n_jobs=-1, cv=cv, random_state=1)","metadata":{"execution":{"iopub.status.busy":"2021-12-29T09:55:36.166866Z","iopub.execute_input":"2021-12-29T09:55:36.167128Z","iopub.status.idle":"2021-12-29T09:55:36.177794Z","shell.execute_reply.started":"2021-12-29T09:55:36.167091Z","shell.execute_reply":"2021-12-29T09:55:36.176661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# execute search\nresult = search.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-12-29T09:55:36.179607Z","iopub.execute_input":"2021-12-29T09:55:36.180232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# summarize result\nprint('Best Score: %s' % result.best_score_)\nprint('Best Hyperparameters: %s' % result.best_params_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### II. Decision Tree","metadata":{}},{"cell_type":"markdown","source":"#### III. Random Forest","metadata":{}},{"cell_type":"markdown","source":"#### IV. Xgboost","metadata":{}}]}